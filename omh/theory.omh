# SPDX-License-Identifier: AGPL-3.0-or-later
# SPDX-FileCopyrightText: University of Washington <https://www.washington.edu>
# SPDX-FileContributor: 2014-22 Bradley M. Bell
# ----------------------------------------------------------------------------
$begin theory$$
$latex \newcommand{\dtheta}[1]{ \frac{\R{d}}{\R{d} \theta_{ #1}} }$$


$section Laplace Approximation for Mixed Effects Models$$
$spell
	CppAD
	cppad
	Kasper Kristensen
	Anders Nielsen
	Casper Berg
	Hans Skaug
	Bradley Bell
$$

$head Reference$$
TMB: Automatic Differentiation and Laplace Approximation,
Kasper Kristensen, Anders Nielsen, Casper W. Berg, Hans Skaug, Bradley M. Bell,
Journal of Statistical Software 70, 1-21 April 2016.

$head Total Likelihood$$
The reference above defines $latex f( \theta, u)$$
to be the negative log-likelihood of the
$latex z$$, $latex y$$, $latex u$$ and $latex \theta$$; i.e.,
$latex \[
- \log [  \;
	\B{p} ( y |  \theta, u ) \B{p} ( u | \theta )  \;
	\B{p} ( z | \theta )\B{p} ( \theta ) \;
]
\] $$


$head Random Likelihood, f(theta, u)$$
We use $latex f( \theta , u )$$ for the part of the likelihood
that depends on the random effects $latex u$$;
$latex \[
	f( \theta, u ) = - \log [ \B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) ]
\] $$

$subhead Assumption$$
The function $latex f(\theta, u)$$ is assumed to be smooth.
Furthermore, there are no constraints on the value of $latex u$$.

$subhead Random Effects Objective$$
Give a value for the fixed effects $latex \theta$$,
the random effects objective is the random likelihood as just
a function of the random effects; i.e., $latex f( \theta , \cdot )$$.

$head Fixed Likelihood, g(theta)$$
We use $latex g( \theta )$$ for the part of the likelihood
that only depends on the fixed effects $latex \theta$$;
$latex \[
	g( \theta ) = - \log [ \B{p} ( z | \theta ) \B{p} ( \theta ) ]
\]$$
The function $latex g( \theta )$$ may not be smooth, to be specific, it
can have absolute values in it (corresponding to the Laplace densities).
Furthermore, there may be  constraints on the value of $latex \theta$$.

$head Optimal Random Effects, u^(theta)$$
Given the fixed effects $latex \theta$$,
we use $latex \hat{u} ( \theta )$$ to denote
the random effects that maximize the random likelihood; i.e.,
$latex \[
	\hat{u} ( \theta ) = \R{argmin} \; f( \theta, u ) \; \R{w.r.t.} \; u
\] $$
Note that this definition agrees with the other definition for
$cref/u^(theta)/cppad_mixed/Notation/Optimal Random Effects, u^(theta)/$$.

$head Objective$$

$subhead Laplace Approximation, h(theta, u)$$
Using the notation above,
the Laplace approximation as a function of both
the fixed and random effects is
$latex \[
h( \theta, u )
=
+ \frac{1}{2} \log \det f_{u,u} ( \theta, u )
+ f( \theta, u )
- \frac{n}{2} \log ( 2 \pi )
\] $$
where $latex n$$ is the number of random effects.

$subhead Laplace Objective, r(theta)$$
We refer to
$latex \[
	r( \theta )
	=
	h[ \theta , \hat{u} ( \theta ) ]
	\approx
	- \log \left[ \int_{-\infty}^{+\infty}
		\B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) \; \B{d} u
	\right]
\] $$
as the Laplace objective.
This corresponds to equation (4) in the
$cref/reference/theory/Reference/$$.

$subhead Fixed Effects Objective, L(theta)$$
The fixed effects objective, as a function of just the fixed effects, is
$latex \[
L ( \theta )
=
r( \theta ) + g( \theta )
\] $$

$head Derivative of Optimal Random Effects$$
Because $latex f(\theta, u)$$ is smooth,
and $latex \hat{u} ( \theta )$$ is optimal w.r.t $latex u$$,
we obtain
$latex \[
	f_u [ \theta , \hat{u} ( \theta ) ] = 0
\] $$
From this equation,
and the implicit function theorem,
it follows that
$latex \[
\hat{u}_\theta ( \theta )
=
- f_{u,u} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u,\theta} \left[ \theta , \hat{u} ( \theta )  \right]
\]$$

$head Derivative of Random Constraints$$
The derivative of the
$cref/random constraint function
	/cppad_mixed
	/Notation
	/Random Constraint Function, A*u^(theta)
/$$
is given by
$latex \[
	\partial_\theta [ A \; \hat{u} ( \theta ) ]
	=
	A \; \hat{u}_\theta ( \theta )
\]$$

$head Derivative of Laplace Objective$$
The derivative of the random part of the objective is given by
$latex \[
r_\theta ( \theta )
=
h_\theta [ \theta , \hat{u} ( \theta ) ]
+
h_u [ \theta , \hat{u} ( \theta ) ] \hat{u}_\theta ( \theta )
\] $$
Thus the derivative of $latex r ( \theta )$$ can be computed
using the derivative of $latex \hat{u} ( \theta )$$
and the partials of $latex h( \theta , u )$$.
Let $latex \partial_k$$ denote the partial with respect to the $th k$$
component of the combined vector $latex ( \theta , u )$$.
$latex \[
\partial_k [ h( \theta , u ) ]
=
\partial_k [ f( \theta , u ) ]
+
\frac{1}{2} \sum_{i=0}^{n-1} \sum_{j=0}^{n-1}
	f_{u,u} ( \theta , u )_{i,j}^{-1}
	\partial_k [ f_{u,u} ( \theta , u)_{i,j} ]
\] $$
where $latex n$$ is the number of random effects.
Note that $latex f_{u,u} ( \theta , u )$$
is often sparse and only non-zero
components need be included in the summation.
This is discussed in more detail near equation (8) in the
$cref/reference/theory/Reference/$$.
We also note that if $latex k$$ corresponds to a component of $latex u$$ then
$latex \[
	\partial_k ( f[ \theta , \hat{u} ( \theta ) ] ) = 0
\] $$

$head Approximate Optimal Random Effects$$

$subhead First Order, U(beta, theta, u)$$
We define  the function
$latex \[
U ( \beta , \theta , u )
=
u - f_{u,u} ( \theta , u )^{-1} f_u ( \beta , u  )
\] $$
It follows that
$latex \[
	U \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\] $$
$latex \[
	U_{\beta} [ \theta , \theta , \hat{u} ( \theta ) ]
	=
	\hat{u}_\theta ( \theta )
\] $$

$subhead Second Order, W(beta, theta, u)$$
We define  the function
$latex \[
W ( \beta , \theta , u )
=
U( \beta , \theta , u )
-
f_{u,u} ( \theta , u )^{-1} f_u [ \beta , U( \beta , \theta , u)  ]
\] $$
It follows that
$latex \[
	W \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\] $$
$latex \[
	W_{\beta} [ \theta , \theta , \hat{u} ( \theta ) ]
	=
	\hat{u}_\theta ( \theta )
\] $$
and for random effects indices $latex i$$,
$latex \[
	W^i_{\beta \beta} [ \theta , \theta , \hat{u} ( \theta ) ]
	=
	\hat{u}^i_{\theta , \theta} ( \theta )
\] $$

$head Approximate Laplace Objective, H(beta, theta, u)$$
Given these facts we define
$latex \[
H( \beta , \theta , u)
=
+ \frac{1}{2} \log \det f_{u,u} [ \beta, W( \beta , \theta , u) ]
+ f[ \beta, U( \beta , \theta , u) ]
- \frac{n}{2} \log ( 2 \pi )
\] $$
It follow that
$latex \[
r_{\theta,\theta} ( \theta )
=
H_{\beta,\beta} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$

$head Approximate Random Constraint Function, B(beta, theta, u)$$
We also define the approximation
$cref/random constraint function
	/cppad_mixed
	/Notation
	/Random Constraint Function, A*u^(theta)
/$$
$latex \[
	B( \beta , \theta , u) = A \; W( \beta , \theta , u )
\] $$

$head Hessian of Laplace Objective$$
Note that the Hessian of the Laplace objective
$latex r_{\theta,\theta} ( \theta ) $$ is required when
$cref/quasi_fixed/derived_ctor/quasi_fixed/$$ is false.
In this case, the representation
$latex \[
r_{\theta,\theta} ( \theta )
=
H_{\beta,\beta} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$
is used to compute this Hessian.

$head Hessian of Random Constraints$$
In the case where
$cref/quasi_fixed/derived_ctor/quasi_fixed/$$ is false
we need to compute second derivatives of the random constraint function.
We use $latex A^i$$ ( $latex B^i $$) to denote one of the
rows of the random constraint matrix
( approximate random constraint function ).
The Hessian of the random constraints can be computed using the formula
$latex \[
	\partial_\theta \partial_\theta [ A^i \; \hat{u} ( \theta ) ]
	=
	B^i_{\beta,\beta} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$

$head Sparse Observed Information$$
Suppose that $latex H$$ is a sparse positive definite Hessian of
a likelihood at the maximum likelihood estimate for its unknown parameters.
The corresponding asymptotic covariance for posterior distribution
of the parameters is normal with covariance $latex H^{-1}$$.
A vector $latex v$$ with this covariance can be simulated as
$latex \[
	v = R w
\] $$
where $latex R$$ is defined by $latex H^{-1} = R R^\R{T}$$ and
$latex w$$ is a normal with mean zero and the identity covariance.
Suppose we have a sparse factorization of the form
$latex \[
	L D L^\R{T} = P H P^\R{T}
\] $$
where $latex L$$ is lower triangular, $latex D$$ is diagonal,
and $latex P$$ is a permutation matrix.
It follows that
$latex \[
	H = P^\R{T} L D L^\R{T} P
\] $$
$latex \[
	H^{-1} = P^\R{T} L^{-\R{T}} D^{-1} L^{-1} P
\] $$
$latex \[
	R = P^\R{T} L^{-\R{T}} D^{-1/2}
\] $$
$latex \[
	v = P^\R{T} L^{-\R{T}} D^{-1/2} w
\] $$
If $latex w$$ is simulated as a normal random vector
with  mean zero and identity covariance,
and $latex v$$ is computed using this formula,
the mean of $latex v$$ is zero and its covariance is given by
$latex \[
	\B{E}[ v v^\R{T} ] = H^{-1}
\] $$

$end
