$Id:$
-----------------------------------------------------------------------------
cppad_mixed: C++ Laplace Approximation of Mixed Effects Models
          Copyright (C) 2014-16 University of Washington
             (Bradley M. Bell bradbell@uw.edu)

This program is distributed under the terms of the
	     GNU Affero General Public License version 3.0 or later
see http://www.gnu.org/licenses/agpl.txt
-----------------------------------------------------------------------------
$begin theory$$
$latex \newcommand{\dtheta}[1]{ \frac{\R{d}}{\R{d} \theta_{ #1}} }$$


$section Laplace Approximation for Mixed Effects Models$$
$spell
	CppAD
	cppad
	Kasper Kristensen
	Anders Nielsen
	Casper Berg
	Hans Skaug
	Bradley Bell
$$

$head Reference$$
TMB: Automatic Differentiation and Laplace Approximation,
Kasper Kristensen, Anders Nielsen, Casper W. Berg, Hans Skaug, Bradley M. Bell,
Journal of Statistical Software, Accepted 2015-02.

$head Total Likelihood$$
The reference above defines $latex f( \theta, u)$$
to be the negative log-likelihood of the
$latex z$$, $latex y$$, $latex u$$ and $latex \theta$$; i.e.,
$latex \[
- \log [  \;
	\B{p} ( y |  \theta, u ) \B{p} ( u | \theta )  \;
	\B{p} ( z | \theta )\B{p} ( \theta ) \;
]
\] $$


$head Random Likelihood, f(theta, u)$$
We use $latex f( \theta , u )$$ for the part of the likelihood
that depends on the random effects $latex u$$;
$latex \[
	f( \theta, u ) = - \log [ \B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) ]
\] $$

$subhead Assumption$$
The function $latex f(\theta, u)$$ is assumed to be smooth.
Furthermore, there are no constraints on the value of $latex u$$.

$head Fixed Likelihood, g(theta)$$
We use $latex g( \theta )$$ for the part of the likelihood
that only depends on the fixed effects $latex \theta$$;
$latex \[
	g( \theta ) = - \log [ \B{p} ( z | \theta ) \B{p} ( \theta ) ]
\]$$
The function $latex g( \theta )$$ may not be smooth, to be specific, it
can have absolute values in it (corresponding to the Laplace densities).
Furthermore, there may be  constraints on the value of $latex \theta$$.

$head Objective$$

$subhead Optimal Random Effects, u^(theta)$$
Given the fixed effects $latex \theta$$,
we use $latex \hat{u} ( \theta )$$ to denote
the random effects that maximize the random likelihood; i.e.,
$latex \[
	\hat{u} ( \theta ) = \R{argmin} \; f( \theta, u ) \; \R{w.r.t.} \; u
\] $$
Note that this definition agrees with the other definition for
$cref/u^(theta)/cppad_mixed/Notation/Optimal Random Effects, u^(theta)/$$.

$subhead Laplace Approximation, h(theta, u)$$
Using the notation above,
the Laplace approximation as a function of both
the fixed and random effects is
$latex \[
h( \theta, u )
=
+ \frac{1}{2} \log \det f_{u,u} ( \theta, u )
+ f( \theta, u )
- \frac{n}{2} \log ( 2 \pi )
\] $$
where $latex n$$ is the number of random effects.

$subhead Random Objective, r(theta)$$
We refer to
$latex \[
	r( \theta )
	=
	h[ \theta , \hat{u} ( \theta ) ]
	\approx
	- \log \left[ \int_{-\infty}^{+\infty}
		\B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) \; \B{d} u
	\right]
\] $$
as the random objective.
This corresponds to equation (4) in the
$cref/reference/theory/Reference/$$.

$subhead Total Objective, L(theta)$$
The total objective, as a function of the fixed effects, is
$latex \[
L ( \theta )
=
r( \theta ) + g( \theta )
\] $$

$head Derivative of Optimal Random Effects$$
Because $latex f(\theta, u)$$ is smooth,
and $latex \hat{u} ( \theta )$$ is optimal w.r.t $latex u$$,
we obtain
$latex \[
	f_u [ \theta , \hat{u} ( \theta ) ] = 0
\] $$
From this equation,
and the implicit function theorem,
it follows that
$latex \[
\hat{u}_\theta ( \theta )
=
- f_{u,u} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u,\theta} \left[ \theta , \hat{u} ( \theta )  \right]
\]$$

$head Derivative of Random Objective$$
The derivative of the random part of the objective is given by
$latex \[
r_\theta ( \theta )
=
h_\theta [ \theta , \hat{u} ( \theta ) ]
+
h_u [ \theta , \hat{u} ( \theta ) ] \hat{u}_\theta ( \theta )
\] $$
Thus the derivative of $latex r ( \theta )$$ can be computed
using the derivative of $latex \hat{u} ( \theta )$$
and the partials of $latex h( \theta , u )$$.
Let $latex \partial_k$$ denote the partial with respect to the $th k$$
component of the combined vector $latex ( \theta , u )$$.
$latex \[
\partial_k [ h( \theta , u ) ]
=
\partial_k [ f( \theta , u ) ]
+
\frac{1}{2} \sum_{i=0}^{n-1} \sum_{j=0}^{n-1}
	f_{u,u} ( \theta , u )_{i,j}^{-1}
	\partial_k [ f_{u,u} ( \theta , u)_{i,j} ]
\] $$
where $latex n$$ is the number of random effects.
Note that $latex f_{u,u} ( \theta , u )$$
is often sparse and only non-zero
components need be included in the summation.
This is discussed in more detail near equation (8) in the
$cref/reference/theory/Reference/$$.
We also note that if $latex k$$ corresponds to a component of $latex u$$ then
$latex \[
	\partial_k ( f[ \theta , \hat{u} ( \theta ) ] ) = 0
\] $$

$head Approximate Optimal Random Effects$$

$subhead First Order, U(beta, theta, u)$$
We define  the function
$latex \[
U ( \beta , \theta , u )
=
u - f_{u,u} ( \theta , u )^{-1} f_u ( \beta , u  )
\] $$
It follows that
$latex \[
	U \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\]$$
and
$latex \[
U_{\beta} \left[ \theta, \theta , \hat{u} ( \theta ) \right]
=
- f_{u,u} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u,\theta} \left[ \theta , \hat{u} ( \theta )  \right]
=
\hat{u}_\theta ( \theta )
\] $$

$subhead Second Order, W(beta, theta, u)$$
We define  the function
$latex \[
W ( \beta , \theta , u )
=
U( \beta , \theta , u )
-
f_{u,u} [ \beta , U( \beta , \theta , u) ]^{-1}
	f_u [ \beta , U( \beta , \theta , u)  ]
\] $$
It follows that
$latex \[
	W \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\] $$
and
$latex \[
W_{\beta} [ \theta , \theta , \hat{u} ( \theta ) ]
=
-
f_{u,u} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u,\theta} [ \beta , \hat{u} ( \theta )  ]
=
\hat{u}_\theta ( \theta )
\] $$
It has been proved that, for $latex i = 0 , \ldots , n-1$$,
$latex \[
	W^i_{\beta \beta} [ \theta , \theta , \hat{u} ( \theta ) ]
	=
	\hat{u}^i_{\theta , \theta} ( \theta )
\] $$

$head Approximate Random Objective, H(beta, theta, u)$$
Given these facts we define
$latex \[
H( \beta , \theta , u)
=
+ \frac{1}{2} \log \det f_{u,u} [ \beta, W( \beta , \theta , u) ]
+ f[ \beta, W( \beta , \theta , u) ]
- \frac{n}{2} \log ( 2 \pi )
\] $$
It follow that
$latex \[
r_{\theta,\theta} ( \theta )
=
H_{\beta,\beta} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$

$head Hessian of Random Objective$$
Note that the Hessian of the random objective
$latex r_{\theta,\theta} ( \theta ) $$ is required when
$cref/quasi_fixed/derived_ctor/quasi_fixed/$$ is false.
In this case, the representation
$latex \[
r_{\theta,\theta} ( \theta )
=
H_{\beta,\beta} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$
is used to compute this Hessian.

$end
